<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-matrix-linear-trans" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>The Matrix of a Linear Transformation</title>
  <introduction>
    <p>
      As we saw in the previous section, linear transformations can be defined using matrices and they can also be defined with no matrices in sight. In this section we will see that, for a certain class of linear transformations, there is <em>always a matrix in sight</em>.
    </p>
  </introduction>

<subsection xml:id="subsec-construct-matrix-lin-trans">
  <title>Constructing the Matrix</title>

  <p>
    Our claim might seem fanciful at first. <em>Can every linear transformation be realized using a matrix?</em> The surprising answer is <em>yes</em>, for a specific kind of linear transformation.
  </p>

  <p>
    We first make an observation related to the definition of the matrix-vector product in <xref ref="examp-matrix-vector-prod"/>.
  </p> 
  
  <note xml:id="note-matrix-times-ej">
    <p>
      If <m>A</m> is an <m>m\times n</m> matrix with columns <m>\mathbf{a}_1, \ldots, \mathbf{a}_n</m>, and if we recall the definition of <m>\mathbf{e}_j</m> from <xref ref="note-define-ej"/>, then 
    <me>
      A(\mathbf{e}_j) = \mathbf{a}_j
    </me>.
    The truth of this equality comes by thinking of <m>A(\mathbf{e}_j)</m> in the way expressed in <xref ref="matrix-vector-def2"/>, as a linear combination of the columns of <m>A</m> with weights from the entries in <m>\mathbf{e}_j</m>.
    </p>
  </note>

  <p>
    We now suppose that <m>\ff</m> is a field and that <m>T:\ff^n \to \ff^m</m> is a linear transformation. We claim that there is a unique <m>m\times n</m> matrix <m>A</m> such that for every <m>\bfv \in \ff^n</m>, <m>T(\bfv) = A\bfv</m>. In other words, we claim that the work of the linear transformation <m>T</m> can be carried out through multiplication by <m>A</m>. 
  </p>

  <p>
    We will define the matrix <m>A</m> which does the job. For each <m>j = 1, \ldots, n</m>, define the vector <m>\mathbf{a}_j</m> by <m>\mathbf{a}_j = T(\mathbf{e}_j)</m>. We then define <m>A</m> as the matrix with columns <m>\mathbf{a}_1, \ldots, \mathbf{a}_n</m>. 
  </p>

  <p>
    Since any vector <m>\bfv \in \ff^n</m>, written as <m>\bfv = [v_i]</m>, has the property that 
    <me>
      \bfv = \sum_{j=1}^n v_j\mathbf{e}_j
    </me>,
    we can verify that the action of <m>T</m> is the same as the action of multiplication by <m>A</m>:
    <me>
      T(\bfv) = T \left( \sum_{j=1}^n v_j \mathbf{e}_j \right) = \sum_{j=1}^n v_j T(\mathbf{e}_j) = \sum_{j=1}^n v_j \mathbf{a}_j = A\bfv
    </me>.
    Note that we used the fact that <m>T</m> is a linear transformation in this last string of equalities.
  </p>

  <p>
    We have just proved the following theorem.
  </p>

  <theorem xml:id="thm-matrix-of-lin-trans">
    <statement>
      <p>
        If <m>T:\ff^n \to \ff^m</m> is a linear transformation, then there exists a unique <m>m\times n</m> matrix <m>A</m> over <m>\ff</m> such that <m>T(\bfv) = A\bfv</m> for all <m>\bfv \in \ff^n</m>.
      </p>
    </statement>
  </theorem>

  <p>
    A scrupulous reader may protest our use of the word <q>unique</q> in the statement of this theorem. Here is the argument concerning uniqueness. If the theorem is true, then (for this theorem) there is only one way it could possibly work. If a matrix <m>A</m> exists, it must have the property that <m>A\mathbf{e}_j = T(\mathbf{e}_j)</m> for all <m>j</m>. Since we have shown that such a construction does work, the matrix <m>A</m> we obtain must be unique.
  </p>
  
  <p>
    This theorem is quite powerful. We will demonstrate that power through two examples that find their origin in <xref ref="sec-linear-trans"/>.
  </p>

  <example xml:id="examp-rotate-reflect-2">
    <statement>
      <p>
        We take our notation from <xref ref="examp-rotate-reflect"/>. Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which reflects a vector in the Cartesian plane across the <m>x</m>-axis, and let <m>S:\rr^2\to\rr^2</m> be the linear transformation which rotates a vector counter-clockwise around the origin by <m>\frac{\pi}{2}</m> radians. In this example we will find the <m>2\times 2</m> matrices <m>A</m> and <m>B</m> such that <m>T(\bfv)=A\bfv</m> and <m>S(\bfv)=B\bfv</m> for all <m>\bfv \in \rr^2</m>. 
      </p>
      <p>
        In the proof of <xref ref="thm-matrix-of-lin-trans"/>, we saw that the way to form the matrix of a linear transformation is to calculate the image of the vectors <m>\mathbf{e}_1, \ldots, \mathbf{e}_n</m>. In this context, we need to calculate the image of <m>\mathbf{e}_1</m> and <m>\mathbf{e}_2</m> under <m>T</m> and <m>S</m>.
      </p>
      <p>
        The calculations we seek are below:
        <me>
          T(\mathbf{e}_1) = \begin{bmatrix}
          1 \\ 0
          \end{bmatrix}, \hspace{6pt}
          T(\mathbf{e}_2) = \begin{bmatrix}
          0 \\ -1
          \end{bmatrix}, \hspace{6pt}
          S(\mathbf{e}_1) = \begin{bmatrix}
          0 \\ 1
          \end{bmatrix}, \hspace{6pt}
          S(\mathbf{e}_2) = \begin{bmatrix}
          -1 \\ 0
          \end{bmatrix}
        </me>.
        This tells us that the matrices <m>A</m> and <m>B</m> are as follows:
        <me>
          A = \begin{bmatrix}
          1 \amp 0 \\ 0 \amp -1
          \end{bmatrix}, \hspace{12pt}
          B = \begin{bmatrix}
          0 \amp -1 \\ 1 \amp 0
          \end{bmatrix}
        </me>.
        Any curious reader can check that these matrices are correct by choosing a vector in <m>\rr^2</m> and multiplying by <m>A</m> and by <m>B</m> separately. The results should align with the actions of <m>T</m> and <m>S</m>, respectively.
      </p>
    </statement>
  </example>
</subsection>

<subsection xml:id="subsec-matrix-mult">
  <title>Composition and Matrix Multiplication</title>
  <p>
    Since linear transformations are functions, we can <em>compose</em> them with other linear transformations. In order for this to make sense, we need to have the codomains and domains match up correctly.
  </p>
  <p>
    If <m>T:U \to V</m> and <m>S:V \to W</m> are linear transformations between vector spaces, then we can define the function <m>S \circ T:U \to W</m> by <m>(S \circ T)(\bfu) = S(T(\bfu))</m> for each <m>\bfu \in U</m>. Note that such a definition is only possible when the <em>codomain of <m>T</m></em> is the same as the <em>domain of <m>S</m></em>. It is not difficult to show that <m>S\circ T</m> is a linear transformation. (See <xref ref="exer-lin-trans-comp"/>.)
  </p>
  <p>
    If <m>U = \ff^n</m>, <m>V = \ff^m</m>, and <m>W = \ff^p</m>, then the linear transformation <m>S \circ T</m> is defined <m>\ff^n \to \ff^p</m>, and <xref ref="thm-matrix-of-lin-trans"/> says that there is a unique matrix over <m>\ff</m> which carries out this linear transformation. What is that matrix? 
  </p>
  <p>
    <xref ref="thm-matrix-of-lin-trans"/> tells us that there are matrices <m>A</m> and <m>B</m> such that the transformations <m>T</m> and <m>S</m> are multiplication by <m>B</m> and <m>A</m>, respectively. The matrix <m>B</m> is <m>m\times n</m> and <m>A</m> is <m>p\times m</m>. We will <em>define</em> the product of <m>A</m> and <m>B</m> so that the matrix of <m>S\circ T</m> is the matrix product <m>AB</m>.
  </p>

  <definition xml:id="def-matrix-mult">
    <statement>
      <p>
        Let <m>A</m> be a <m>p\times m</m> matrix over a field <m>\ff</m> and let <m>B</m> be an <m>m\times n</m> matrix over <m>\ff</m>. Then the <term>matrix product</term> <m>AB</m> is the unique <m>p\times n</m> matrix over <m>\ff</m> such that for all <m>\bfu \in \ff^n</m>, 
        <me>
          A(B\bfu) = (AB)\bfu
        </me>.
      </p>
    </statement>
  </definition>

  <note xml:id="matrix-dimensions-multiplication">
    <p>
      When we take the matrix product <m>AB</m>, the number of columns of <m>A</m> must match the number of rows of <m>B</m>. The matrix product makes no sense (and cannot be computed) otherwise. The matrix <m>AB</m> then has the same number of rows as <m>A</m> and the same number of columns as <m>B</m>.
    </p>
  </note>

  <p>
    Though we have defined matrix multiplication in terms of the composition of linear transformations, we can multiply matrices of the correct dimensions even when we have no specific linear transformations in mind. This is similar to our understanding of row-reducing a matrix<mdash></mdash>this arose in the context of solving linear systems, but the process can be carried out on any matrix.
  </p>

<p>
  We have defined matrix multiplication, but we have not specified how the entries in the matrix product are calculated. Fear not; the wait is over.
</p>

<p>
  We will use the definition of matrix multiplication and the formula we have for the product of a matrix and a vector (see <xref ref="form-matrix-vect-prod"/>). Since <m>(AB)\bfu</m> is a vector, we will record a formula for entry <m>i</m> in this vector. In what follows, we assume <m>A=[a_{ij}]</m> and the entries of <m>B=[b_{ij}]</m>; we also assume <m>\bfu = [u_i]</m>:
  <md>
    <mrow>[(AB)\bfu]_i \amp = [A(B\bfu)]_i = \sum_{k=1}^m a_{ik} [B\bfu]_k</mrow> 
    <mrow> \amp = \sum_{k=1}^m a_{ik} \sum_{j=1}^n b_{kj}u_j = \sum_{j=1}^n \left(\sum_{k=1}^m a_{ik}  b_{kj} \right) u_j </mrow>
  </md>.
</p>

<p>
  When we look again at the formula in <xref ref="form-matrix-vect-prod"/> for the product of a matrix and a vector, we see that 
  <men xml:id="form-matrix-product-entry">
    [AB]_{ij} = \sum_{k=1}^m a_{ik}b_{kj}
  </men>
  for all <m>1 \le i \le p</m> and all <m>1 \le j \le n</m>. In words, this means that the <m>(i,j)</m>-entry of <m>AB</m> is the entry-wise product of row <m>i</m> in <m>A</m> with column <m>j</m> in <m>B</m>. (Later we will acknowledge this as the <em>dot product</em> of two vectors in <m>\ff^m</m>.)
</p>

<p>
  We will try to make this concrete with some examples.
</p>

<example>
  <statement>
    <p>
      Let <m>A</m> and <m>B</m> be the following matrices over <m>\rr</m>:
      <me>
        A = \begin{bmatrix}
        2 \amp -1 \\ 3 \amp 4
        \end{bmatrix}, \hspace{12pt}
        B = \begin{bmatrix}
        -2 \amp 0 \\ 1 \amp -3
        \end{bmatrix}
      </me>.
      Note that the product <m>AB</m> makes sense since the number of columns of <m>A</m> is the same as the number of rows of <m>B</m>. Here is the matrix product:
      <me>
        AB = \begin{bmatrix}
        2(-2)-1(1) \amp 2(0)-1(-3) \\ 3(-2) + 4(1) \amp 3(0)+4(-3)
        \end{bmatrix} = 
        \begin{bmatrix}
        -5 \amp 3 \\ -2 \amp -12
        \end{bmatrix}
      </me>.      
      Since the sizes of <m>A</m> and <m>B</m> allow it, we can also calculate <m>BA</m> in this example:
      <me>
        BA = \begin{bmatrix}
        -4 \amp 2 \\ -7 \amp -13
        \end{bmatrix}
      </me>.
      Finally, we observe that <m>AB \neq BA</m>.
    </p>
  </statement>
</example>

<example>
  <statement>
    <p>
      Let <m>A</m> and <m>B</m> be the following matrices over <m>\ff_5</m>:
      <me>
        A = \begin{bmatrix}
        4 \amp 0 \\ 1 \amp 4 \\ 3 \amp 0
        \end{bmatrix}, \hspace{12pt}
        B = \begin{bmatrix}
        3 \amp 3 \\ 4 \amp 2
        \end{bmatrix}
      </me>.
      Since <m>A</m> is <m>3\times 2</m> and <m>B</m> is <m>2\times 2</m>, we can calculate <m>AB</m>, which will be <m>3\times 2</m>. (In this example we cannot calculate <m>BA</m>.)
      Here is the matrix product:
      <me>
        AB = \begin{bmatrix}
        4(3) + 0(4) \amp 4(3)+0(2) \\ 1(3)+4(4) \amp 1(3)+4(2) \\ 3(3)+0(4) \amp 3(3)+0(2)
        \end{bmatrix} = 
        \begin{bmatrix}
        2 \amp 2 \\ 4 \amp 1 \\ 4 \amp 4
        \end{bmatrix}
      </me>.
      To obtain the last equality, we remember that we are working in <m>\ff_5</m>.
    </p>
  </statement>
</example>

<p>
  Since we defined matrix multiplication in the context of the composition of linear transformations, our next example picks up on this theme.
</p>

<example>
  <statement>
    <p>
      We return to <xref ref="examp-rotate-reflect-2"/> and consider the linear transformations <m>S,T:\rr^2 \to \rr^2</m>, where <m>T</m> reflects a vector in the Cartesian plane across the <m>x</m>-axis and <m>S</m> rotates a vector counter-clockwise around the origin by <m>\frac{\pi}{2}</m> radians. In the previous example, we calculated the matrices <m>A</m> and <m>B</m> for <m>T</m> and <m>S</m>, respectively. What is the matrix for <m>S\circ T</m>?
    </p>
    <p>
      We have defined matrix multiplication to answer exactly this question. We only need to multiply the matrices in the proper order. The matrix for <m>S\circ T</m> is
      <me>
        BA = \begin{bmatrix}
        0 \amp -1 \\ 1 \amp 0
        \end{bmatrix} \begin{bmatrix}
        1 \amp 0 \\ 0 \amp -1
        \end{bmatrix} = 
        \begin{bmatrix}
        0 \amp 1 \\ 1 \amp 0
        \end{bmatrix}
      </me>.
      A related question in this context is whether or not linear transformations <em>commute</em>. In other words, is <m>S\circ T = T\circ S</m>? For this example, answering that question boils down to comparing the matrix product <m>AB</m> with the product <m>BA</m> which we have just calculated:
      <me>
        AB = \begin{bmatrix}
        1 \amp 0 \\ 0 \amp -1
        \end{bmatrix} \begin{bmatrix}
        0 \amp -1 \\ 1 \amp 0
        \end{bmatrix}  = 
        \begin{bmatrix}
        0 \amp -1 \\ -1 \amp 0
        \end{bmatrix}
      </me>.
      From this we can see that <m>S\circ T</m> and <m>T\circ S</m> are two distinct linear transformations.
    </p>
  </statement>
</example>

<p>
  As we start to deal more regularly with matrices in the context of linear transformations, we need to recall the notation <m>M_{m,n}(\ff)</m> and <m>M_n(\ff)</m> from <xref ref="matrices-vspace"/>.
</p>
<!-- 
<note>
  <p>
    For a field <m>\ff</m>, the notation <m>M_{m,n}(\ff)</m> denotes the set of all <m>m\times n</m> matrices over <m>\ff</m>. When <m>m=n</m>, we will use the notation <m>M_n(\ff)</m> instead of <m>M_{n,n}(\ff)</m>.
  </p>
</note> -->

<p>
  The next theorem records some facts about matrix multiplication which will be useful later in the text. We will walk the reader through the proof of this theorem in the exercises at the end of this section.
</p>

<theorem xml:id="thm-matrix-mult-facts">
  <statement>
    <p>
      Let <m>A, A_1, A_2 \in M_{m,n}(\ff)</m>, <m>B, B_1, B_2 \in M_{n,p}(\ff)</m>, and <m>C \in M_{p,q}(\ff)</m>. Then 
      <ol>
        <li>
          <p>
            <m>A(BC) = (AB)C</m>,
          </p>
        </li>
        <li>
          <p>
            <m>A(B_1 + B_2) = AB_1 + AB_2</m>, and
          </p>
        </li>
        <li>
          <p>
            <m>(A_1+A_2)B = A_1B + A_2B</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>

<p>
  This theorem says that, if all of the matrix products make sense, matrix multiplication is associative and obeys both of the distributive laws.
</p>

<p>
  There is one other useful way to think about matrix multiplication<mdash></mdash>in terms of the columns of the matrix. 
</p>

<proposition xml:id="prop-matrix-mult">
  <statement>
    <p>
      Let <m>A \in M_{m,n}(\ff)</m>, <m>B \in M_{n,p}(\ff)</m>, and let the columns of <m>B</m> be <m>\mathbf{b}_1,\ldots,\mathbf{b}_p</m>. Then the columns of <m>AB</m> are <m>A\mathbf{b}_1,\ldots,A\mathbf{b}_p</m>.
    </p>
  </statement>
  <proof>
    <p>
      By our definition of the matrix product, for each <m>j=1, \ldots, p</m> we have 
      <me>
        (AB)\mathbf{e}_j = A(B\mathbf{e}_j)
      </me>.
      The observation in <xref ref="note-matrix-times-ej"/> means that <m>B\mathbf{e}_j=\mathbf{b}_j</m>, so we have 
      <me>
        (AB)\mathbf{e}_j = A \mathbf{b}_j
      </me>.
      Since <m>(AB)\mathbf{e}_j</m> is the <m>j</m>th column of <m>AB</m>, this proves the proposition.
    </p>
  </proof>
</proposition>

<p>
  From the understanding we developed in <xref ref="examp-matrix-vector-prod"/>, this proposition means that every column of the matrix product <m>AB</m> is a linear combination of the columns of <m>A</m>, when the product <m>AB</m> is defined.
</p>
</subsection>

<subsection xml:id="subsec-linear-trans-invertible">
  <title>Invertible Matrices</title>
  <p>
    With matrix multiplication defined in terms of the composition of linear transformations, we turn to a specific composition in this subsection. Specifically, we will think about matrices for linear transformations <m>S</m> and <m>T</m> when <m>S\circ T</m> is the identity transformation. 
  </p>
  <p>
    In <xref ref="def-lin-trans-invertible"/>, we called such linear transformations <em>invertible</em>. When such a linear transformation can be accomplished by matrix multiplication, we will refer to the connected matrix using this same term.
  </p>

  <definition xml:id="def-invertible-matrix">
    <statement>
      <p>
        Let <m>A \in M_n(\ff)</m>. The matrix <m>B</m> is an <term>inverse</term> matrix for <m>A</m> if 
        <me>
          AB = BA = I_n
        </me>.        
      </p>
      <p>
        If a matrix <m>A</m> has an inverse, we say that <m>A</m> is <term>invertible</term> or <term>non-singular</term>. If <m>A</m> is a matrix for which no inverse matrix exists, we say that <m>A</m> is <term>singular</term> or not invertible.
      </p>
    </statement>
  </definition>

  <p>
    It may strike the reader as strange that only square matrices have a chance at being invertible<mdash></mdash>we have only defined invertibility for square matrices. There is a good reason for this, which we will explore in the exercises. 
  </p>

  <p>
    If we know an inverse <m>B</m> of a matrix <m>A</m>, then we can solve matrix-vector equations with ease:
    <md>
      <mrow>A\bfx \amp = \bfy</mrow>
      <mrow>B(A\bfx) \amp = B\bfy</mrow>
      <mrow>(BA)\bfx \amp = B\bfy</mrow>
      <mrow>I \bfx \amp = B\bfy</mrow>
      <mrow>\bfx \amp = B\bfy</mrow>
    </md>.
  </p>

  <p>
    The next three propositions present some properties of matrix inverses.
  </p>

  <proposition xml:id="prop-matrix-inverse-unique">
    <statement>
      <p>
        If a matrix <m>A \in M_n(\ff)</m> has an inverse, that inverse is unique.
      </p>
    </statement>
    <proof>
      <p>
        Let <m>A \in M_n(\ff)</m> and suppose that both <m>B,C \in M_n(\ff)</m> are inverses for <m>A</m>. Then we have 
        <me>
          B = BI_n = B(AC) = (BA)C = I_nC = C
        </me>.        
      </p>
    </proof>
  </proposition>

  <p>
    This proposition allows us to refer to <em>the inverse</em> of a matrix <m>A</m> and to use the notation <m>A^{-1}</m> for that matrix.
  </p>

  <proposition xml:id="prop-matrix-inverse-product">
    <statement>
      <p>
        Suppose that <m>A,B \in M_n(\ff)</m> are both invertible. Then <m>AB</m> is invertible as well and 
        <me>
          (AB)^{-1} = B^{-1}A^{-1}
        </me>.        
      </p>
    </statement>
    <proof>
      <p>
        If <m>A</m> and <m>B</m> are both invertible, then both <m>A^{-1}</m> and <m>B^{-1}</m> exist. Since matrix multiplication is associative, the following calculations show that the matrix <m>B^{-1}A^{-1}</m> satisfies the properties of the inverse of <m>AB</m>, thereby making <m>AB</m> invertible:
        <md>
          <mrow>(AB)(B^{-1}A^{-1}) \amp = A(BB^{-1})A^{-1} = A(I_n)A^{-1} = AA^{-1} = I_n;</mrow>
          <mrow>(B^{-1}A^{-1})(AB) \amp = B^{-1}(A^{-1}A)B = B^{-1}(I_n)B = B^{-1}B = I_n</mrow>
        </md>.
      </p>
    </proof>
  </proposition>

  <p>
    This final proposition states what may seem like an obvious fact, but which should still be justified. That justification is left to the exercises.
  </p>

  <proposition xml:id="prop-matrix-inverse-involution">
    <statement>
      <p>
        Let <m>A \in M_n(\ff)</m> be invertible. Then <m>A^{-1}</m> is also invertible and <m>(A^{-1})^{-1} = A</m>.
      </p>
    </statement>
  </proposition>

  <p>
    While we are not yet ready to calculate the inverse of a matrix (stay tuned!), we can provide examples of invertible matrices and their inverses.
  </p>

  <example xml:id="examp-inverse-matrices">
    <statement>
      <p>
        Consider the following matrix <m>A \in M_2(\rr)</m>:
        <me>
          A = \begin{bmatrix}
          3 \amp 5 \\ 1 \amp 2
          \end{bmatrix}
        </me>.
        We can verify that <m>A^{-1}</m> is 
        <me>
          A^{-1} = \begin{bmatrix}
          2 \amp -5 \\ -1 \amp 3
          \end{bmatrix}
        </me>
        by computing the product in both orders and verifying that the result is <m>I_2</m> in both cases.
      </p>
      <p>
        Similarly, here is a <m>3\times 3</m> matrix over <m>\ff_3</m> which is invertible:
        <me>
          B = \begin{bmatrix}
          1 \amp 2 \amp 1 \\ 0 \amp 2 \amp 0 \\ 1 \amp 2 \amp 0
          \end{bmatrix}
        </me>.
        The reader is encouraged to verify that the following matrix satisfies the properties of the inverse of <m>B</m>:
        <me>
          B^{-1} = \begin{bmatrix}
          0 \amp 2 \amp 1 \\ 0 \amp 2 \amp 0 \\ 1 \amp 0 \amp 2
          \end{bmatrix}
        </me>.        
      </p>
    </statement>
  </example>
</subsection>

<subsection xml:id="subsec-matrix-transpose">
  <title>The Transpose of a Matrix</title>
  
  <p>
    The <em>transpose</em> of a matrix is useful notation for some formulas that will appear later.
  </p>
  <definition xml:id="def-transpose">
    <statement>
      <p>
        If <m>A \in M_{m,n}(\ff)</m>, then the <term>transpose</term> of <m>A</m>, denoted <m>A^T</m>, is the element of <m>M_{n,m}(\ff)</m> whose rows are the columns of <m>A</m>. In other words, 
        <me>
          [A^T]_{ij} = [A]_{ji}
        </me>
        for all <m>1 \le i \le n</m> and all <m>1 \le j \le m</m>.
      </p>
    </statement>
  </definition>

  <note>
    <p>
      The transpose is an easy way to turn a column vector into a row vector and vice versa. 
    </p>
  </note>

  <example>
    <statement>
      <p>
        If <m>A</m> is the <m>2\times 3</m> matrix 
        <me>
          A = \begin{bmatrix}
          2 \amp -1 \amp 0 \\ -2 \amp 4 \amp 5
          \end{bmatrix}
        </me>,
        then <m>A^T</m> is the <m>3\times 2</m> matrix 
        <me>
          A^T = \begin{bmatrix}
          2 \amp -2 \\ -1 \amp 4 \\ 0 \amp 5
          \end{bmatrix}
        </me>.        
      </p>
    </statement>
  </example>

  <p>
    Some matrices are unaffected by taking the transpose. These deserve a special designation!
  </p>

  <definition xml:id="def-symmetric-matrix">
    <statement>
      <p>
        A matrix which is equal to its own transpose is called a <term>symmetric matrix</term>. (All symmetric matrices must be square by necessity.)
      </p>
    </statement>
  </definition>

  <p>
    The following theorem collects some properties related to the transpose of a matrix. 
    <!-- (Note that this theorem references the sum and scalar multiple of a matrix; the reader should understand these operations in the same way that addition and scalar multiplication of vectors in <m>\ff^n</m> are understood.) -->
  </p>

  <theorem xml:id="thm-properties-transpose">
    <statement>
      <p>
        Let <m>A, C \in M_{m,n}(\ff)</m>, let <m>B \in M_{n,p}(\ff)</m>, and let <m>k \in \ff</m>. Then the following properties hold: 
        <ol>
          <li>
            <p>
              <m>(A^T)^T = A</m>;
            </p>
          </li>
          <li>
            <p>
              <m>(A+C)^T = A^T + C^T</m>;
            </p>
          </li>
          <li>
            <p>
              <m>(kA)^T = kA^T</m>; and 
            </p>
          </li>
          <li>
            <p>
              <m>(AB)^T = B^TA^T</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <p>
        The first three parts of this theorem are immediate from the definitions and require no proof. To prove the fourth part, we will compare the <m>(i,j)</m>-entry of both <m>(AB)^T</m> and <m>B^TA^T</m>. First, from the definition of the transpose and <xref ref="form-matrix-product-entry"/> we see that 
        <me>
          [(AB^T)]_{ij} = [AB]_{ji} = \sum_{k=1}^n a_{jk}b_{ki}
        </me>.
        To compare, entry <m>(i,j)</m> of <m>B^TA^T</m> is 
        <me>
          [B^TA^T]_{ij} = \sum_{k=1}^n [B^T]_{ik}[A^T]_{kj} = \sum_{k=1}^n b_{ki}a_{jk}
        </me>.
        Since multiplication is commutative in fields, these two expressions are equal.
      </p>
    </proof>
  </theorem>

  <note>
    <p>
      While it might be more aesthetically pleasing if we did not have to switch the order of the multiplication when taking the transpose of a product, this type of formula makes sense when considering the dimensions of the matrices involved. If <m>A</m> is <m>m\times n</m> and <m>B</m> is <m>n\times p</m>, then the expression <m>A^TB^T</m> wouldn't even make sense unless <m>m=p</m>. Further, reversing the order in a formula involving matrix multiplication is typical, as we have already seen in <xref ref="prop-matrix-inverse-product"/>.
    </p>
  </note>
</subsection>

<reading-questions>
  <exercise>
  <statement>
    <p>
      Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which is rotation <em>clockwise</em> around the origin by <m>\frac{\pi}{2}</m> radians. Find the matrix for <m>T</m>. (Refer to <xref ref="examp-rotate-reflect-2"/>.) Explain your process.
    </p>
  </statement>
</exercise>
<exercise> 
<statement>
  <p>
    Consider the following two matrices <m>A</m> and <m>B</m> over <m>\rr</m>:
    <me>
      A = \begin{bmatrix}
      0 \amp 3 \\ 5 \amp -1 \\ -1 \amp -3
      \end{bmatrix}, \hspace{12pt}
      B = \begin{bmatrix}
      -3 \amp -2 \amp 4 \\ 0 \amp 2 \amp -1
      \end{bmatrix}
    </me>.
    Calculate both <m>AB</m> and <m>BA</m>.
  </p>
</statement>
</exercise>
<exercise> 
<statement>
  <p>
    Write down a <m>3\times 3</m> matrix over <m>\ff_5</m> which is symmetric. (See <xref ref="def-symmetric-matrix"/>.)
  </p>
</statement>
</exercise>
</reading-questions>

<exercises>
  <exercise>
    <statement>
      <p>
        Let <m>A</m>, <m>B</m>, and <m>C</m> be the following matrices over <m>\rr</m>:
        <me>
          A = \begin{bmatrix}
          2 \amp 0 \amp 1 \\ -1 \amp -2 \amp 2
          \end{bmatrix}, \hspace{6pt}
          B = \begin{bmatrix}
          3 \amp -3 \\ 2 \amp 1
          \end{bmatrix}, \hspace{6pt}
          C = \begin{bmatrix}
          0 \amp 4 \amp 1 \\ 3 \amp 2 \amp -2 \\ 4 \amp -3 \amp 3
          \end{bmatrix}
        </me>.
        For each of the following, determine whether the given calculation makes sense. If it does, find the requested matrix. (Do this by hand, without technology.) If it doesn't make sense, explain why it doesn't.
        <ol>
          <li>
            <p>
              <m>A^2</m>
            </p>
          </li>
          <li>
            <p>
              <m>AB</m>
            </p>
          </li>
          <li>
            <p>
              <m>AC</m>
            </p>
          </li>
          <li>
            <p>
              <m>BC</m>
            </p>
          </li>
          <li>
            <p>
              <m>BA</m>
            </p>
          </li>
          <li>
            <p>
              <m>B^2</m>
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which reflects a vector across the line <m>y=x</m>. Find the matrix for <m>T</m>.
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which projects a vector onto the line <m>y=x</m>. Find the matrix for <m>T</m>.
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which projects a vector onto the line <m>y=-x</m>. Find the matrix for <m>T</m>.
      </p>
    </statement>
  </exercise>
  <!-- <exercise>
    <statement>
      <p>
        Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which projects a vector onto the line <m>y=kx</m> for <m>k \in \rr</m>. Find the matrix for <m>T</m>.
      </p>
    </statement>
  </exercise> -->
  <exercise>
    <statement>
      <p>
        Let <m>T:\rr^2 \to \rr^2</m> be the linear transformation which rotates a vector counter-clockwise around the origin by an angle of <m>\theta</m> radians. Find the matrix for <m>T</m>. (Each entry in the matrix should be an expression involving <m>\theta</m>.)
      </p>
    </statement>
  </exercise>
  <subexercises>
  <title>Writing Exercises</title>  
  <exercise>
    <statement>
      <p>
        Let <m>A \in M_n(\ff)</m> be invertible. Prove that <m>A^T</m> is invertible and that <m>(A^T)^{-1} = (A^{-1})^T</m>.
      </p>
    </statement>
  </exercise>
  <!-- <exercise>
    <statement>
      <p>
        Prove theorem 3.2.10.
      </p>
    </statement>
  </exercise> -->
  <exercise>
    <statement>
      <p>
        Let <m>A \in M_{m,n}(\ff)</m>. 
        <ol>
          <li>
            <p>
              Suppose that <m>A</m> is <term>left-invertible</term>, meaning that there is an <m>n\times m</m> matrix <m>B</m> such that <m>BA=I_n</m>. Prove that <m>m \ge n</m>. 
            </p>
          </li>
          <li>
            <p>
              Suppose that <m>A</m> is <term>right-invertible</term>, meaning that there is an <m>n \times m</m> matrix <m>B</m> such that <m>AB = I_m</m>. Prove that <m>m \le n</m>.
            </p>
          </li>
          <li>
            <p>
              Prove that any <m>A</m> which is invertible must be a square matrix.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Prove <xref ref="prop-matrix-inverse-involution"/>. 
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        In fields, we have the <em>cancellation law</em> for multiplication. If <m>ab=ac</m>  and <m>a \neq 0</m>, then <m>b=c</m>. Does matrix multiplication have this property?
      </p>
      <p>
        Let <m>A</m>, <m>B</m>, and <m>C</m> be matrices over <m>\ff</m> such that <m>AB</m> and <m>AC</m> make sense and are the same size and <m>A</m> is not the zero matrix. If <m>AB=AC</m>, must it be true that <m>B=C</m>? Either prove this is true or provide a counter-example.
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        In fields, multiplication has the <em>no zero divisors</em> property. If <m>xy=0</m>, then either <m>x=0</m> or <m>y=0</m>. Does matrix multiplication have this property?
      </p>
      <p>
        Let <m>A</m> and <m>B</m> be matrices over <m>\ff</m> such that <m>AB</m> makes sense. Let <m>Z</m> be the matrix of the same size as <m>AB</m> consisting of all zeros. If <m>AB = Z</m>, must it be true that either <m>A</m> or <m>B</m> is a matrix of all zeros? Either prove this is true or provide a counter-example.
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Let <m>A \in M_2(\ff_5)</m> be of the form 
        <me>
          A = \begin{bmatrix}
          a \amp 0 \\ b \amp c
          \end{bmatrix}
        </me>.
        <ol>
          <li>
            <p>
              What conditions must <m>a</m>, <m>b</m>, and <m>c</m> satisfy so that <m>A^2 = I_2</m>?
            </p>
          </li>
          <li>
            <p>
              How many matrices in <m>M_2(\ff_5)</m> of this form have the property that <m>A^2=I_2</m>?
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </exercise>
</subexercises>
</exercises> 


</section>