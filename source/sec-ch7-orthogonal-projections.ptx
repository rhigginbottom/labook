<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-orthogonal-projections" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Orthogonal Projections</title>
  <introduction>
    <p>
      An inner product provides the tool to decompose vectors into useful components. We have already seen this in <xref ref="lem-pair-orthog"/>, but in this section we will expand our discussion. The process of orthogonal projection opens the door to many applications.
    </p>
  </introduction>

<subsection xml:id="subsec-orthog-complements">
  <title>Orthogonal Complements</title>

  <p>
    In an inner product space, we can collect all vectors orthogonal to any given set of vectors. In particular, we can do this with a subspace.
  </p>

  <definition xml:id="def-orthog-complement">
    <statement>
      <p>
        Let <m>U</m> be a subspace of an inner product space <m>V</m>. Then the <term>orthogonal complement</term> of <m>U</m>, denoted <m>U^{\perp}</m>, is defined as
        <me>
          U^{\perp} = \{ \bfv \in V \mid \ip{\bfu,\bfv} = 0 \text{ for all } \bfu \in U \}
        </me>.        
      </p>
    </statement>
  </definition>

<p>
  It is relatively easy to verify that <m>U^{\perp}</m> is itself a subspace of <m>V</m>. We will leave that proof as an exercise.
</p>

<proposition xml:id="prop-orthog-comp-subspace">
  <statement>
    <p>
      If <m>U</m> is a subspace of an inner product space <m>V</m>, then <m>U^{\perp}</m> is also a subspace of <m>V</m>. 
    </p>
  </statement>
</proposition>
  
<p>
  The easiest examples of orthogonal complements to visualize are in <m>\rr^2</m> and <m>\rr^3</m>. If <m>L</m> is a line through the origin in <m>\rr^2</m>, then <m>L^{\perp}</m> is the line perpendicular to <m>L</m> which passes through the origin. If <m>P</m> is a plane through the origin in <m>\rr^3</m>, then <m>P^{\perp}</m> is the line through the origin which is perpendicular to <m>P</m>. 
</p>

<p>
  In an inner product space, any vector can be uniquely decomposed with reference to a subspace and its orthogonal complement.
</p>

<theorem xml:id="thm-decomp-orthog-comp">
  <statement>
    <p>
      Let <m>V</m> be an inner product space and let <m>U</m> be a finite-dimensional subspace of <m>V</m>. Then every vector <m>\bfv \in V</m> can be uniquely written in the form 
      <me>
        \bfv = \bfu + \bfw
      </me>,
      where <m>\bfu \in U</m> and <m>\bfw \in U^{\perp}</m>.       
    </p>
  </statement>
  <proof>
    <p>
      Since <m>U</m> is finite-dimensional, there is an orthonormal basis for <m>U</m>, <m>\{ \bfe_1, \ldots, \bfe_m \}</m>. For any <m>\bfv \in V</m>, we define <m>\bfu</m> by 
      <me>
        \bfu = \sum_{i=1}^m \ip{\bfv, \bfe_i} \bfe_i
      </me>,
      and we let <m>\bfw = \bfv - \bfu</m>. Then we have <m>\bfv = \bfu + \bfw</m> and <m>\bfu \in U</m>. For each <m>k</m>, we have 
      <me>
        \ip{\bfu, \bfe_k} = \ip{\bfv, \bfe_k}
      </me>,
      so 
      <me>
        \ip{\bfw, \bfe_k} = \ip{\bfv, \bfe_k} - \ip{\bfu, \bfe_k} = 0
      </me>.
      Since <m>\bfw</m> is orthogonal to each element of the orthonormal basis of <m>U</m>, we have <m>\bfw \in U^{\perp}</m>. 
    </p>
    <p>
      We now need to prove that <m>\bfu</m> and <m>\bfw</m> are unique. Suppose that <m>\bfu_1, \bfu_2 \in U</m> and <m>\bfw_1, \bfw_2 \in U^{\perp}</m> such that 
      <me>
        \bfu_1 + \bfw_1 = \bfu_2 + \bfw_2
      </me>.
      We consider the vector <m>\bfx</m>, 
      <me>
        \bfx = \bfu_1 - \bfu_2 = \bfw_2 - \bfw_1
      </me>.
      Since <m>U</m> and <m>U^{\perp}</m> are subspaces, we have <m>\bfx \in U</m> and <m>\bfx \in U^{\perp}</m>, which means that <m>\ip{\bfx, \bfx} = 0</m>. This means that <m>\bfx = \bfo</m>, so that <m>\bfu_1 = \bfu_2</m> and <m>\bfw_1 = \bfw_2</m>. 
    </p>
  </proof>
</theorem>

<!-- <note xml:id="note-orth-proj">
  <p>
    One part of the proof of this theorem deserves special mention. The way we constructed <m>\bfu</m> in the first paragraph of this proof will always work. Namely, if <m>\{\bfe_1, \ldots, \bfe_m\}</m> is an orthonormal basis for a subspace <m>U</m> of <m>V</m>, then the way to calculate <m>\proj_U(\bfv)</m> for any <m>\bfv \in V</m> is 
    <me>
      \proj_U(\bfv) = \sum_{i=1}^m \ip{\bfv, \bfe_i} \bfe_i
    </me>.    
  </p>
</note> -->

</subsection>


<subsection xml:id="subsec-orthog-proj">
  <title>Orthogonal Projections</title>
  
<p>
  Once we have the sort of decomposition that <xref ref="thm-decomp-orthog-comp"/> provides, we can properly talk about orthogonal projections.
</p>

<definition xml:id="def-orthog-proj">
  <statement>
    <p>
      Let <m>U</m> be a subspace of an inner product space <m>V</m>. The <term>orthogonal projection</term> onto <m>U</m> is the function <m>\proj_U:V \to V</m> given by <m>\proj_U(\bfv) = \bfu</m>, where <m>\bfv = \bfu + \bfw</m> for <m>\bfu \in U</m> and <m>\bfw \in U^{\perp}</m>. 
    </p>
  </statement>
</definition>

<p>
  Orthogonal projection has some important properties which we now collect in the following theorem.
</p>

<theorem xml:id="thm-alg-props-orthog-proj">
  <statement>
    <p>
      Let <m>U</m> be a finite-dimensional subspace of an inner product space <m>V</m>. 
      <ol>
        <li>
          <p>
            The function <m>\proj_U</m> is a linear transformation.
          </p>
        </li>
        <li>
          <p>
            If <m>\{\bfe_1, \ldots, \bfe_n \}</m> is an orthonormal basis of <m>U</m>, then 
            <me>
              \proj_U(\bfv) = \sum_{i=1}^n \ip{\bfv, \bfe_i}\bfe_i
            </me>
            for each <m>\bfv \in V</m>.
          </p>
        </li>
        <li>
          <p>
            For each <m>\bfv \in V</m>, <m>\bfv - \proj_U(\bfv) \in U^{\perp}</m>. 
          </p>
        </li>
        <li>
          <p>
            For each <m>\bfv, \bfw \in V</m>, 
            <me>
              \ip{ \proj_U(\bfv), \bfw} = \ip{\proj_U(\bfv), \proj_U(\bfw)} = \ip{\bfv, \proj_U(\bfw)}
            </me>.             
          </p>
        </li>
        <li>
          <p>
            If <m>\mcb = \{ \bfe_1, \ldots, \bfe_m \}</m> is an orthonormal basis for <m>V</m>, and <m>\{ \bfe_1, \ldots, \bfe_n \}</m> an orthonormal basis of  <m>U</m> (with <m>n \le m</m>), then <m>[\proj_U]_{\mcb}</m> is a diagonal matrix with the first <m>n</m> diagonal entries being 1 and the remaining diagonal entries being 0.
          </p>
        </li>
        <li>
          <p>
            We have <m>\range(\proj_U) = U</m> and <m>\proj_U(\bfu) = \bfu</m> for all <m>\bfu \in U</m>. 
          </p>
        </li>
        <li>
          <p>
            We have <m>\kerr(\proj_U) = U^{\perp}</m>. 
          </p>
        </li>
        <li>
          <p>
            If <m>V</m> is finite-dimensional, then <m>\proj_{U^{\perp}} = I - \proj_U</m>. 
          </p>
        </li>
        <li>
          <p>
            We have <m>(\proj_U)^2 = \proj_U</m>. 
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <proof>
    <p>
      We will prove property 1. Let <m>\bfv_1, \bfv_2 \in V</m>, so we can write <m>\bfv_1 = \bfu_1 + \bfw_1</m> and <m>\bfv_2 = \bfu_2 + \bfw_2</m>, for <m>\bfu_1, \bfu_2 \in U</m> and <m>\bfw_1,\bfw_2 \in U^{\perp}</m>. Then 
      <me>
        \bfv_1 + \bfv_2 = (\bfu_1 + \bfu_2) + (\bfw_1 + \bfw_2)
      </me>,
      which tells us that 
      <me>
        \proj_U(\bfv_1 + \bfv_2) = \bfu_1 + \bfu_2 = \proj_U(\bfv_1) + \proj_U(\bfv_2)
      </me>. 
      We now let <m>c \in \ff</m> and <m>\bfv \in V</m>. We write <m>\bfv = \bfu + \bfw</m>, with <m>\bfu \in U</m> and <m>\bfw \in U^{\perp}</m>. We note that <m>c\bfu \in U</m> and <m>c \bfw \in U^{\perp}</m> since <m>U</m> and <m>U^{\perp}</m> are subspaces. Then
      <me>
        \proj(c\bfv) = \proj(c\bfu + c\bfw) = c\bfu = c\proj_U(\bfv)
      </me>.
      This proves that <m>\proj_U</m> is a linear transformation. 
    </p>
    <p>
      We will also prove property 3. Let <m>\bfv \in V</m> and write <m>\bfv = \bfu + \bfw</m>, with <m>\bfu \in U</m> and <m>\bfw \in U^{\perp}</m>. Then 
      <me>
        \bfv - \proj_U(\bfv) = \bfv - \bfu = \bfw \in U^{\perp}
      </me>.      
    </p>
    <p>
      We leave the proof of the other properties to the exercises.
    </p>
  </proof>
</theorem>

<p>
  We can use part of this theorem to describe the matrix of <m>\proj_U</m> explicitly. 
</p>

<proposition xml:id="prop-matrix-of-proj-1">
  <statement>
    <p>
      Let <m>U</m> be a subspace of <m>\rr^n</m> or <m>\cc^n</m> with orthonormal basis <m>\{ \bfv_1, \ldots, \bfv_m \}</m>. Then the matrix of <m>\proj_U</m> with respect to the standard basis <m>\mce</m> is 
      <me>
        [\proj_U]_{\mce} = \sum_{i=1}^m \bfv_i \bfv_i^*
      </me>,
      where <m>A^*</m> denotes the conjugate transpose of a matrix. 
    </p>
  </statement>
  <proof>
    <p>
      This fact follows from part 2 of <xref ref="thm-alg-props-orthog-proj"/> and the fact that the standard inner product in <m>\cc^n</m> can be written as 
      <me>
        \ip{\bfu, \bfv} = \bfv^* \bfu
      </me>,
      where matrix multiplication is in view on the right side of the equals sign.
    </p>
  </proof>
</proposition>

<p>
  Lest this endeavor become purely speculative, we now carry out an example.
</p>

<example>
  <statement>
    <p>
      We consider the plane through the origin in <m>\rr^3</m> defined by <m>x + 2y - z = 0</m>. This is a subspace of <m>\rr^3</m>, let's call it <m>U</m>, and we can identify the following basis: 
      <me>
        \mcb = \left\{ \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} \right\}
      </me>.
      We use the Gram-Schmidt process on this basis to produce this orthonormal basis of <m>U</m>: 
      <me>
         \left\{ \begin{bmatrix} -\frac{2}{\sqrt{5}} \\[6pt] \frac{1}{\sqrt{5}} \\[6pt] 0 \end{bmatrix}, \begin{bmatrix} \frac{1}{\sqrt{30}} \\[6pt] \frac{2}{\sqrt{30}} \\[6pt] \frac{5}{\sqrt{30}} \end{bmatrix} \right\}
      </me>.
      Using this orthonormal basis of <m>U</m>, we can write the matrix of <m>\proj_U</m> with respect to the standard basis. We have
      <md>
        <mrow>[\proj_U]_{\mce} \amp = \bfe_1\bfe_1^* + \bfe_2\bfe_2^*</mrow>
        <mrow> \amp = \begin{bmatrix} \frac{4}{5} \amp -\frac{2}{5} \amp 0 \\[6pt] 
        -\frac{2}{5} \amp \frac{1}{5} \amp 0 \\[6pt] 
        0 \amp 0 \amp 0 \end{bmatrix} + 
        \begin{bmatrix} \frac{1}{30} \amp \frac{1}{15} \amp \frac{1}{6} \\[6pt] 
        \frac{1}{15} \amp \frac{2}{15} \amp \frac{1}{3} \\[6pt] 
        \frac{1}{6} \amp \frac{1}{3} \amp \frac{5}{6} \end{bmatrix}</mrow>
        <mrow> \amp = \begin{bmatrix} \frac{5}{6} \amp -\frac{1}{3} \amp \frac{1}{6} \\[6pt]
        -\frac{1}{3} \amp \frac{1}{3} \amp \frac{1}{3} \\[6pt]
        \frac{1}{6} \amp \frac{1}{3} \amp \frac{5}{6} \end{bmatrix}
        </mrow>
      </md>. 
    </p>
    <p>
      To finish this example, we will decompose a specific vector <m>\bfv</m> into the pieces specified by <xref ref="thm-decomp-orthog-comp"/>. Let 
      <me>
        \bfv = \begin{bmatrix} 3 \\ -6 \\ 2 \end{bmatrix}
      </me>.
      We note that <m>\bfv \not \in U</m>. Now we calculate <m>\proj_U(\bfv)</m>:
      <me>
        \proj_U(\bfv) = \begin{bmatrix} \frac{5}{6} \amp -\frac{1}{3} \amp \frac{1}{6} \\[6pt]
        -\frac{1}{3} \amp \frac{1}{3} \amp \frac{1}{3} \\[6pt]
        \frac{1}{6} \amp \frac{1}{3} \amp \frac{5}{6} \end{bmatrix} \begin{bmatrix} 3 \\ -6 \\ 2 \end{bmatrix} = 
        \begin{bmatrix} \frac{29}{6} \\[6pt] -\frac{7}{3} \\[6pt] \frac{1}{6} \end{bmatrix}
      </me>.
      Then <m>\bfw = \bfv - \proj_U(\bfv)</m>, so 
      <me>
        \bfw = \begin{bmatrix} -\frac{11}{6} \\[6pt] -\frac{11}{3} \\[6pt] \frac{11}{6} \end{bmatrix}
      </me>.
      This completes the decomposition <m>\bfv = \bfu + \bfw</m>. 
    </p>
  </statement>
</example>

<p>
  <xref ref="prop-matrix-of-proj-1"/> depended on having an orthonormal basis for the subspace <m>U</m>. We can always find such a basis through the Gram-Schmidt process, but there is an alternative way to produce the matrix for orthogonal projection.
</p>

<proposition xml:id="prop-matrix-of-proj-2">
  <statement>
    <p>
      Let <m>U</m> be a subspace of <m>\rr^n</m> or <m>\cc^n</m> with basis <m>\{ \bfv_1, \ldots, \bfv_k \}</m>. Let <m>A</m> be the <m>n \times k</m> matrix with columns <m>\bfv_1, \ldots, \bfv_k</m>. Then 
      <me>
        [\proj_U]_{\mce} = A(A^*A)^{-1}A^*
      </me>.      
    </p>
  </statement>
  <proof>
    <p>
      We note that <m>\proj_U(\bfx)</m> is an element of <m>U</m>, so it can be written as a linear combination of the columns of <m>A</m>. In other words, there is a vector <m>\bfx'</m> which satisfies <m>\proj_U(\bfx) = A\bfx'</m>. By part 3 of <xref ref="thm-alg-props-orthog-proj"/>, 
      <me>
        \bfx - \proj_U(\bfx) = \bfx - A\bfx' \in U^{\perp}
      </me>.
      Specifically, we have 
      <me>
        \ip{\bfx - A\bfx', \bfv_j} = \bfv_j^*(\bfx - A\bfx') = 0
      </me>
      for each <m>\bfv_j</m>. If we rewrite these <m>k</m> equations in matrix form, we have 
      <me>
        A^*(\bfx - A\bfx') = \bfo
      </me>
      or <m>A^*A \bfx' = A^*\bfx</m>. If <m>A^*A</m> is invertible, then we can multiply both sides of this equation by <m>A(A^*A)^{-1}</m>, and we get 
      <me>
        A\bfx' = A(A^*A)^{-1}A^* \bfx
      </me>.
      This completes the proof, since <m>A\bfx' = \proj_U(\bfx)</m>. 
    </p>
    <p>
      In the last paragraph we assumed that <m>A^*A</m> was invertible, so we now prove that fact. We can do this by proving that the null space of <m>A^*A</m> is trivial. Suppose that <m>A^*A\bfx = \bfo</m>, so we have 
      <me>
        0 = \ip{A^*A\bfx, \bfx} = \bfx^*(A^*A\bfx) = (A\bfx)^*(A\bfx) = \vnorm{A\bfx}^2
      </me>.
      But since the columns of <m>A</m> are linearly independent (they are basis vectors for <m>U</m>), <m>A</m> must have rank <m>k</m>. By the Rank-Nullity Theorem (<xref ref="thm-rank-nullity"/>), this means that <m>\dim(\nll(A)) = 0</m>, and since <m>\vnorm{A\bfx}^2 = 0</m> means <m>A\bfx = \bfo</m>, we must have <m>\bfx = \bfo</m>. This proves that <m>A^*A</m> is invertible.
    </p>
  </proof>
</proposition>

<p>
  In the following theorem we capture two important geometric properties of orthogonal projections. 
</p>

<theorem xml:id="thm-geom-props-orthog-projs">
  <statement>
    <p>
      Let <m>U</m> be a finite-dimensional subspace of an inner product space <m>V</m>. 
      <ol>
        <li>
          <p>
            For each <m>\bfv \in V</m>, <m>\vnorm{\proj_U(\bfv)} \le \vnorm{\bfv}</m>, with equality if and only if <m>\bfv \in U</m>. 
          </p>
        </li>
        <li>
          <p>
            For each <m>\bfv \in V</m> and each <m>\bfu \in U</m>, we have 
            <me>
              \vnorm{\bfv - \proj_U(\bfv)} \le \vnorm{\bfv - \bfu}
            </me>,
            with equality if and only if <m>\bfu = \proj_U(\bfv)</m>. 
          </p>
        </li>
      </ol>
    </p>
  </statement>
  <proof>
    <p>
      Since <m>\bfv - \proj_U(\bfv) \in U^{\perp}</m> by part 3 of <xref ref="thm-alg-props-orthog-proj"/>, <m>\bfv - \proj_U(\bfv)</m> is orthogonal to <m>\proj_U(\bfv)</m>, so, using <xref ref="thm-pythag-inner-prods"/> we have 
      <md>
        <mrow>\vnorm{\bfv}^2 \amp = \vnorm{\proj_U(\bfv) + (\bfv - \proj_U(\bfv))}^2</mrow>
        <mrow> \amp = \vnorm{\proj_U(\bfv)}^2 + \vnorm{\bfv - \proj_U(\bfv)}^2</mrow>
        <mrow> \amp \ge \vnorm{\proj_U(\bfv)}^2</mrow>
      </md>.
      Equality holds here if and only if <m>\vnorm{\bfv - \proj_U(\bfv)}^2 = 0</m>, which is true if and only if <m>\bfv = \proj_U(\bfv)</m>. This only happens if <m>\bfv \in U</m>.
    </p>
    <p>
      We now move on to the second part of the theorem. We know that <m>\bfv - \proj_U(\bfv) \in U^{\perp}</m> and that <m>\proj_U(\bfv) - \bfu \in U</m>, so 
      <md>
        <mrow>\vnorm{\bfv - \bfu}^2 \amp = \vnorm{(\bfv - \proj_U(\bfv)) + (\proj_U(\bfv) - \bfu)}^2</mrow>
        <mrow> \amp = \vnorm{(\bfv - \proj_U(\bfv))}^2 + \vnorm{(\proj_U(\bfv) - \bfu)}^2</mrow>
        <mrow> \amp \ge \vnorm{(\bfv - \proj_U(\bfv))}^2 </mrow>
      </md>.
      We have equality here if and only if <m>\vnorm{(\proj_U(\bfv) - \bfu)}^2 = 0</m>, which happens if and only if <m>\proj_U(\bfv) = \bfu</m>; that is, if and only if <m>\bfv \in U</m>. 
    </p>
  </proof>
</theorem>

<note>
  <p>
    This theorem says that, first, orthogonal projections result in a <em>shorter</em> vector. That is, orthogonal projection is a type of <em>contraction</em>. Secondly, <m>\proj_U(\bfv)</m> is the closest vector in <m>U</m> to the vector <m>\bfv</m>.
  </p>
</note>

<p>
  Finding the closest vector to <m>\bfv</m> in a subspace <m>U</m> can be thought of as giving the best approximation of <m>\bfv</m> by elements of <m>U</m>. This leads to our application of least squares approximation.
</p>

</subsection>
  
<subsection xml:id="subsec-least-squares">
  <title>Least Squares Approximation</title>
  
<p>
  We consider a set of points <m>\{(x_i, y_i)\}</m> in <m>\rr^2</m>; in practice, these are usually the result of data collection, perhaps a sample of two numeric variables from a population. A graph of such points is called a <em>scatterplot</em>, and we often want to find the <q>line of best fit</q> for these data. There are many ways to measure <q>best fit,</q> and our method here will be the least squares linear regression technique. 
</p>

<p>
  Define a subspace <m>U</m> of <m>\rr^n</m> in the following way, where the <m>x_i</m> in the definition are the <m>x</m>-coordinates of the data: 
  <me>
    U = \left\{ \begin{bmatrix} mx_i + b \end{bmatrix} \in \rr^n \mid m, b \in \rr \right\}
  </me>.
  If we let <m>\mathbf{1}</m> denote the <m>n\times 1</m> vector where each entry is 1, then 
  <me>
    U = \{ m\bfx + b\mathbf{1} \mid m, b \in \rr \}
  </me>,
  where <m>\bfx</m> is the vector of all of the first coordinates in our data set.
</p>

<p>
  The points <m>(x_i,y_i)</m> all lie on a single line <m>y = mx+b</m> if and only if the vector <m>\bfy</m> of second coordinates of our data lies in <m>U</m>. This does not happen often, as <m>U</m> is only a two-dimensional subspace of <m>\rr^n</m>. So, we want to find the closest point in <m>U</m> to <m>\bfy</m><mdash></mdash>by <xref ref="thm-geom-props-orthog-projs"/>, we can find this through orthogonal projection. When we find <m>\proj_U(\bfy)</m>, the <m>m</m> and <m>b</m> will give us our equation of the regression line.
</p>

<p>
  This is called a <q>least squares</q> regression, because minimizing the distance from <m>\bfy</m> to <m>U</m> involves minimizing a distance. This distance in <m>\rr^n</m>, when using the dot product, looks like a sum of squares. 
</p>

<example>
  <statement>
    <p>
      Consider the following set of five points in <m>\rr^2</m>: <me>\{(2,1), (1,0), (4,4), (4,5), (3,2) \}</me>. Our subspace <m>U \subseteq \rr^5</m> is spanned by <m>\bfx</m> and <m>\mathbf{1}</m>, where <m>\bfx</m> is the vector of first coordinates 
      <me>
        \bfx = \begin{bmatrix} 2 \\ 1 \\ 4 \\ 4 \\ 3 \end{bmatrix}
      </me>.
      We form the <m>5 \times 2</m> matrix <m>A</m> with columns <m>\bfx</m> and <m>\mathbf{1}</m>. Then, by <xref ref="prop-matrix-of-proj-2"/>, we have 
      <me>
        [\proj_U]_{\mce} \bfy = A(A^*A)^{-1}A^* \bfy
      </me>,
      where <m>\bfy</m> is the <m>5\times 1</m> vector of the <m>y</m>-coordinates of our data. 
    </p>
    <p>
      We calculate the following: 
      <me>
        A^*A = \begin{bmatrix} 46 \amp 14 \\ 14 \amp 5 \end{bmatrix}, \hspace{12pt}
        (A^*A)^{-1} = \begin{bmatrix} \frac{5}{34} \amp -\frac{7}{17} \\[6pt] -\frac{7}{17} \amp \frac{23}{17} \end{bmatrix}
      </me>.
      Now, we don't actually want <m>\proj_U(\bfy)</m>, because that is a vector in <m>\rr^5</m>. We want to know the coefficients <m>m</m> and <m>b</m> in the linear combination of the column vectors of <m>A</m> which produce <m>\proj_U(\bfy)</m>. In other words, we want the vector
      <me>
        \bfw = (A^*A)^{-1}A^* \bfy = \begin{bmatrix} \frac{26}{17} \\[6pt] -\frac{32}{17} \end{bmatrix}
      </me>. 
      Since <m>\proj_U(\bfy) = A\bfw</m>, this means that <m>m = \frac{26}{17}</m> and <m>b = -\frac{32}{17}</m>. We can see that this is a believable solution by looking at the graph below which contains the five points as well as the line <m>y = \frac{26}{17}x - \frac{32}{17}</m>. 
    </p>
    <figure>
      <image width="40%" xml:id="least-squares-regression">
      <latex-image>
      <description>Graph of regression line with five points</description>
        \begin{tikzpicture}
          \begin{axis}[axis lines=center, x=.75cm, y=.75cm, xmin=-1,ymin=-1,xmax=6,ymax=6,
          axis line style = {line width=1pt, color=lightgray,Stealth-Stealth}, %xlabel={$x$}, ylabel={$y$},
          label style={font=\scriptsize},tick label style={font=\scriptsize}, major tick length=4pt, xtick align=outside,
          ytick align=outside,
          xtick={1,2,...,5}, ytick={1,2,...,5}]
			    \node at (2,1) [circle, fill, inner sep=1.5pt]{};          
          \node at (1,0) [circle, fill, inner sep=1.5pt]{};          
          \node at (3,2) [circle, fill, inner sep=1.5pt]{};          
          \node at (4,4) [circle, fill, inner sep=1.5pt]{};          
          \node at (4,5) [circle, fill, inner sep=1.5pt]{};      
          \addplot [domain=1:5, line width = 1pt, color = red] {(26/17)*x - (32/17)};    
          \end{axis}
        \end{tikzpicture}
      </latex-image>       
      </image>
    </figure>
  </statement>
</example>

</subsection>

<reading-questions>
  <exercise>
  <statement>
    <p>
      Let <m>L</m> be the line <m>y = 5x</m> in <m>\rr^2</m>. 
      <ol>
        <li>
          <p>
            Calculate an orthonormal basis for <m>L</m>. (We are considering <m>\rr^2</m> with the usual dot product.)
          </p>
          <!-- <p>
            <m>\bfe_1 = \begin{bmatrix} \frac{1}{\sqrt{26}} \\ \frac{5}{\sqrt{26}} \end{bmatrix}</m>
          </p> -->
        </li>
        <li>
          <p>
            Let <m>\bfv = \begin{bmatrix} 2 \\ 3 \end{bmatrix}</m>. Using part 2 of  <xref ref="thm-alg-props-orthog-proj"/>, calculate <m>\proj_L(\bfv)</m>. 
          </p>
          <!-- <p>
            <m>\proj_L(\bfv) = \begin{bmatrix} \frac{17}{26} \\ \frac{85}{26} \end{bmatrix}</m>
          </p> -->
        </li>
      </ol>
    </p>
  </statement>
</exercise>
<exercise> 
<statement>
  <p>
    Repeat the previous reading question if the inner product on <m>\rr^2</m> is 
    <me>
      \ip{\bfu, \bfv} = 2u_1v_1 + u_2v_2
    </me>.    
  </p>
  <!-- <p>
    <m>\bfe_1 = \begin{bmatrix} \frac{1}{\sqrt{27}} \\ \frac{5}{\sqrt{27}} \end{bmatrix}</m>
    <m>\proj_L(\bfv) = \begin{bmatrix} \frac{19}{27} \\ \frac{95}{27} \end{bmatrix}</m>
  </p> -->
</statement>
</exercise>
</reading-questions>

<exercises>
  <exercise>
    <statement>
      <p>
        Let <m>L</m> be the line <m>y = \frac{3}{5}x</m> in <m>\rr^2</m>. Write the vector <m>\bfv = \begin{bmatrix} -1 \\ 4 \end{bmatrix}</m> as the sum of a vector in <m>L</m> and a vector in <m>L^{\perp}</m>. (Use the standard dot product as the inner product in <m>\rr^2</m>.)
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Let <m>U = \spn\{\bfv_1, \bfv_2 \}</m>, where 
        <me>
          \bfv_1 = \begin{bmatrix} 1 \\ -2 \\ 4 \end{bmatrix}, \hspace{6pt}
          \bfv_2 = \begin{bmatrix} 0 \\ 1 \\ 3 \end{bmatrix}
        </me>.
        <ol>
          <li>
            <p>
              Find the matrix <m>[\proj_U]_{\mce}</m>. (Use the standard dot product as the inner product in <m>\rr^3</m>.)
            </p>
          </li>
          <li>
            <p>
              Using your work from part (a), find the vector in <m>U</m> which is closest to <m>\bfv</m>, if 
              <me>
                \bfv = \begin{bmatrix} -2 \\ 0 \\ 5 \end{bmatrix}
              </me>.              
            </p>
          </li>
        </ol>        
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Consider the following inner product on <m>P_2</m>: 
        <me>
          \ip{p, q} = p(-1)q(-1) + p(0)q(0) + p(1)q(1)
        </me>.
        Let <m>U = \spn\{t - t^2, 1 + 2t\}</m>. If <m>p = 2 - t + 2t^2</m>, write <m>p</m> as the sum of a vector in <m>U</m> and a vector in <m>U^{\perp}</m>. 
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Consider the following four points in <m>\rr^2</m>: 
        <me>
          (-1, -1), (1,2), (2, 0.5), (-0.75, 1)
        </me>.
        Find the least-squares regression line for these points. 
      </p> <!-- y = (2/5)x + 1/2 -->
    </statement>
  </exercise>
  <subexercises>
    <title>Writing Exercises</title>  
    <exercise>
      <statement>
        <p>
          Let <m>U</m> be a subspace of an inner product space <m>V</m>. Prove that <m>U^{\perp}</m> is a subspace of <m>V</m>. 
        </p>
      </statement>
    </exercise>
    <exercise>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix. Prove that <m>(\row(A))^{\perp} = \nll(A)</m>. 
        </p> <!-- Lay, Thm 3, p.337 -->
      </statement>
    </exercise>
  </subexercises>
</exercises> 


</section>